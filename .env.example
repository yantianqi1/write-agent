# LLM API Configuration
# Provider selection: openai, anthropic, gemini, azure-openai, ollama
LLM_PROVIDER=openai

# OpenAI / OpenAI-compatible API
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4

# Anthropic Claude (optional)
# ANTHROPIC_API_KEY=your_key_here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Google Gemini (optional)
# GEMINI_API_KEY=your_gemini_api_key_here
# GEMINI_MODEL=gemini-2.0-flash-exp

# Azure OpenAI (optional)
# AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_DEPLOYMENT=gpt-4

# Ollama (local LLM)
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2

# Database Configuration
# Database file path (relative or absolute)
DB_DATABASE_PATH=./data/writeagent.db

# Database connection pool settings
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10
DB_POOL_TIMEOUT=30
DB_POOL_RECYCLE=3600

# Enable SQL query logging (for debugging)
DB_ECHO_SQL=false

# Redis Configuration (optional - for production)
REDIS_URL=redis://localhost:6379/0
REDIS_PASSWORD=
REDIS_DB=0

# PostgreSQL Configuration (for production/Docker)
# DATABASE_URL=postgresql://user:password@localhost:5432/writeagent
POSTGRES_PASSWORD=writeagent_pass

# Cache Configuration
CACHE_BACKEND=redis  # 'memory' or 'redis' - 生产环境推荐使用 redis
CACHE_TTL=1800  # 30 minutes

# Redis connection (for distributed caching)
# 本地开发: redis://localhost:6379/0
# Docker: redis://redis:6379/0
# 生产环境: redis://[:password@]host:port/db
REDIS_URL=redis://localhost:6379/0

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# CORS Configuration
ALLOWED_ORIGINS=*

# JWT Authentication
# 生成强密钥: python -c "import secrets; print(secrets.token_urlsafe(32))"
JWT_SECRET_KEY=change-this-to-a-strong-secret-key-in-production
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=10080
# 设置为 'true' 启用全局认证（除健康检查和文档外的所有 API 都需要认证）
REQUIRE_AUTH=false

# Sentry Error Tracking (optional)
# Get DSN from https://sentry.io/
# SENTRY_DSN=https://examplePublicKey@o0.ingest.sentry.io/0
# ENVIRONMENT=production
